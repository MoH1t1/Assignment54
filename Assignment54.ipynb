{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "# Q1. Describe the decision tree classifier algorithm and how it works to make predictions.\n\nThe decision tree classifier builds a tree-like structure where each internal node represents a feature or attribute, each branch represents a decision rule, and each leaf node \nrepresents a class label. The algorithm splits the data based on feature values that result in the best separation of the target classes, using criteria like Gini impurity or entropy. \nPredictions are made by traversing the tree from the root to a leaf, following the decisions at each node.\n\n# Q2. Provide a step-by-step explanation of the mathematical intuition behind decision tree classification.\n\nChoose a feature: Select the feature that best separates the classes based on a criterion like Gini impurity or entropy.\nSplit the data: Divide the data into subsets based on feature values.\nEvaluate the quality of the split: Calculate the Gini impurity or information gain (entropy) for each split.\nRecursion: Repeat the process recursively for each subset until stopping criteria are met (e.g., depth limit or all data points in a node belong to the same class).\nMake predictions: Assign the majority class from the leaf node to new data points based on the feature values.\n    \n# Q3. Explain how a decision tree classifier can be used to solve a binary classification problem.\n    \nIn binary classification, the decision tree algorithm recursively splits the data based on the feature that provides the best separation of the two classes. The algorithm uses \nmeasures like Gini impurity or entropy to evaluate splits and creates a tree where the leaves represent either of the two classes. For a new data point, the tree is traversed \nfrom the root to a leaf, and the class label of the leaf node is assigned.\n\n# Q4. Discuss the geometric intuition behind decision tree classification and how it can be used to make predictions.\n\nGeometrically, a decision tree divides the feature space into rectangular regions, with each region corresponding to a class. Each split creates a decision boundary that separates\ndata points of different classes. As the tree grows deeper, the space is partitioned into more regions, and each region corresponds to a specific class. Predictions are made by\nidentifying the region in which the input point falls.\n\n# Q5. Define the confusion matrix and describe how it can be used to evaluate the performance of a classification model.\n    \nA confusion matrix is a table used to evaluate the performance of a classification model by comparing the actual labels with the predicted labels. It includes:\n\nTrue Positive (TP): Correctly predicted positive instances.\nTrue Negative (TN): Correctly predicted negative instances.\nFalse Positive (FP): Incorrectly predicted as positive.\nFalse Negative (FN): Incorrectly predicted as negative.\nThe matrix helps in calculating various performance metrics, such as accuracy, precision, recall, and F1 score.\n\n# Q7. Discuss the importance of choosing an appropriate evaluation metric for a classification problem and explain how this can be done.\n    \nChoosing the right evaluation metric is crucial because different metrics highlight different aspects of model performance:\n\nAccuracy is useful when classes are balanced.\nPrecision is important when false positives are costly (e.g., spam detection).\nRecall is important when false negatives are costly (e.g., medical diagnoses).\nF1 Score balances precision and recall when both are important.\nThe choice depends on the problem context, class distribution, and the consequences of different types of errors.\n\n# Q8. Provide an example of a classification problem where precision is the most important metric, and explain why.\n    \nIn email spam classification, precision is important because falsely classifying a legitimate email as spam (false positive) can cause inconvenience. Minimizing false positives\nensures that users don’t miss important emails.\n\n# Q9. Provide an example of a classification problem where recall is the most important metric, and explain why.\n\nIn medical diagnoses for diseases like cancer, recall is crucial because missing a positive case (false negative) could have severe consequences. Ensuring that all potential \npositive cases are detected (even at the cost of some false positives) is more important to avoid underdiagnosis.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Q6. Provide an example of a confusion matrix and explain how precision, recall, and F1 score can be calculated from it.\n\nExample Confusion Matrix:\n                         Predicted Positive\t   Predicted Negative\nActual Positive\t             50 (TP)\t           10 (FN)\nActual Negative            \t 5 (FP)\t               100 (TN)\n\nPrecision = TP/TP+FP\n          = 50/ 50+5\n          = 0.909\n\nRecall = TP/TP+FN \n       = 50/50+10 \n       = 0.833\n\nF!  = 2 X Precision×Recall/ Precision+Recall\n    =  2× 0.909×0.833/0.909+0.833\n    =  0.869",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}